# Working Memory

## **Phase 1: Strengthening the Foundations**  
### **1. Improve Vector and Matrix Operations**
- âœ… **Current:** Basic `Vector` and `Matrix` classes implemented with tests.  
- âœ… **Next Steps:**  
  - Optimize matrix operations (**dot product, transpose, determinant, inverse, row/column operations**). 
  - Implement **broadcasting** for element-wise operations.  
  - Implement **efficient memory management** (e.g., avoiding unnecessary copies).  
- ðŸ”² Update Test cases
- ðŸ”² Update README

### **2. Enhance the Perceptron**
- âœ… **Current:** Basic perceptron implemented with tests.  
- ðŸ”² **Next Steps:**  
  - Modify perceptron to accept **batch updates** for training efficiency.  
  - Generalize perceptron to support **multi-class classification** (One-vs-All or Softmax).  
- ðŸ”² Update Test cases
- ðŸ”² Update README

---

## **Phase 2: Expanding Classic Machine Learning Models**
### **3. Implementing Linear Regression**
- ðŸ”² Implement **Ordinary Least Squares (OLS)** regression.  
- ðŸ”² Implement **Gradient Descent-based Linear Regression** for large datasets.  
- ðŸ”² Update README

### **4. Implementing Logistic Regression**
- ðŸ”² Implement **Logistic Regression for binary classification**.  
- ðŸ”² Extend to **multi-class classification using Softmax Regression**.  
- ðŸ”² Implement **L1 and L2 regularization** (Lasso & Ridge Regression).  
- ðŸ”² Update README

### **5. Support Vector Machines (SVMs)**
- ðŸ”² Implement **hard-margin and soft-margin SVMs**.  
- ðŸ”² Implement **Quadratic Programming Solver** (or use an optimization technique like SMO).  
- ðŸ”² Implement **Kernel SVMs with RBF, polynomial, and linear kernels**.  
- ðŸ”² Update README

### **6. Implement Kernel PCA**
- ðŸ”² Implement **Principal Component Analysis (PCA)** for dimensionality reduction.  
- ðŸ”² Implement **Kernel PCA** with support for RBF, polynomial, and custom kernels.  
- ðŸ”² Update README
---

## **Phase 3: Neural Networks (MLPs) and Deep Learning**
### **7. Implementing a Multi-Layer Perceptron (MLP)**
- ðŸ”² Implement a **Multi-Layer Perceptron (MLP)** class with:  
  - Support for multiple layers (hidden layers).  
  - Configurable **activation functions**:  
    - Sigmoid, Tanh, ReLU, LeakyReLU, Softmax.  
  - Matrix-based forward propagation for efficiency.  
- ðŸ”² Update README

### **8. Backpropagation & Optimization**
- ðŸ”² Implement **backpropagation**:  
  - Compute gradients using **chain rule**.  
  - Update weights using **gradient descent**.  
- ðŸ”² Implement **learning rate schedules** (e.g., step decay, exponential decay).  
- ðŸ”² Add support for **SGD, Momentum, Adam, RMSprop** optimizers.  
- ðŸ”² Update README

### **9. Loss Functions**
- ðŸ”² Implement standard **loss functions**:  
  - Mean Squared Error (MSE) (for regression).  
  - Cross-Entropy Loss (for classification).  
  - Hinge Loss (for SVM-like models).  
- ðŸ”² Update README
---

## **Phase 4: Regularization and Model Generalization**
### **10. Regularization Techniques**
- ðŸ”² Implement **L1/L2 regularization (weight decay)**.  
- ðŸ”² Implement **dropout** to improve generalization.  
- ðŸ”² Update README
### **11. Model Evaluation & Metrics**
- ðŸ”² Implement **accuracy, precision, recall, F1-score, confusion matrix**.  
- ðŸ”² Support **validation & test splits** for training models properly.  
- ðŸ”² Update README
---

## **Phase 5: Expanding Model Capabilities**
### **12. Implementing Convolutional Neural Networks (CNNs)**
- ðŸ”² Implement **convolution layers** for image processing.  
- ðŸ”² Implement **max pooling & average pooling**.  
- ðŸ”² Implement **basic image preprocessing** (normalization, grayscale conversion).  
- ðŸ”² Update README

### **13. Implementing Recurrent Neural Networks (RNNs)**
- ðŸ”² Implement **basic RNN cell**.  
- ðŸ”² Implement **Long Short-Term Memory (LSTM) & Gated Recurrent Units (GRU)**.  
- ðŸ”² Update README
---

## **Phase 6: Performance & Efficiency**
### **14. Optimization & Parallelization**
- ðŸ”² Implement **automatic differentiation** for gradient computation.  
- ðŸ”² Optimize matrix computations using **parallelization (multi-threading or SIMD)**.  
- ðŸ”² Consider **GPU acceleration** with Rust libraries like `wgpu` or `cust` (CUDA bindings).  
- ðŸ”² Update README
---