# Working Memory

## **Phase 1: Strengthening the Foundations**  
### **1. Improve Vector and Matrix Operations**  
- âœ… **Current:** Basic `Vector` and `Matrix` classes implemented with tests.  
- âœ… **Next Steps:**  
  - Optimize matrix operations (**dot product, transpose, determinant, inverse, row/column operations**).  
  - Implement **broadcasting** for element-wise operations.  
  - Implement **efficient memory management** (e.g., avoiding unnecessary copies).  

### **2. Enhance the Perceptron**  
- âœ… **Current:** Basic perceptron implemented with tests.  
- âœ… **Next Steps:**  
  - Modify perceptron to accept **batch updates** for training efficiency.  
  - Generalize perceptron to support **multi-class classification** (One-vs-All or Softmax).  

### **3. Housekeeping for Phase 1**  
- âœ… Update Test cases  
- âœ… Update README  

---

## **Phase 2: Expanding Classic Machine Learning Models**  
### **3. Implementing Linear Regression**  
- âœ… Implement **Ordinary Least Squares (OLS) Closed-Form Solution** regression.  
- âœ… Implement **Gradient Descent-based Linear Regression** for large datasets.  
- âœ… Update README  

### **4. Implementing Logistic Regression**  
- âœ… Implement **Logistic Regression for binary classification**.  
- âœ… Extend to **multi-class classification using Softmax Regression**.  
- âœ… Implement **L1 and L2 regularization** (Lasso & Ridge Regression).  
- ðŸ”² Update README  


### **5. Support Vector Machines (SVMs)**
- âœ… Implement **hard-margin and soft-margin SVMs**.  
- âœ… Implement **Quadratic Programming Solver** (or use an optimization technique like SMO).  
- ðŸ”² Implement **Kernel SVMs with RBF, polynomial, and linear kernels**.  
- ðŸ”² Update README

### **6. Implement Kernel PCA**  
- ðŸ”² Implement **Principal Component Analysis (PCA)** for dimensionality reduction.  
- ðŸ”² Implement **Kernel PCA** with support for RBF, polynomial, and custom kernels.  
- ðŸ”² Update README  

---

## **Phase 3: Neural Networks (MLPs) and Deep Learning**  
### **7. Implementing a Multi-Layer Perceptron (MLP)**  
- ðŸ”² Implement a **Multi-Layer Perceptron (MLP)** class with:  
  - Support for multiple layers (hidden layers).  
  - Configurable **activation functions**:  
    - Sigmoid, Tanh, ReLU, LeakyReLU, Softmax.  
  - Matrix-based forward propagation for efficiency.  
- ðŸ”² Update README  

### **8. Backpropagation & Optimization**  
- ðŸ”² Implement **backpropagation**:  
  - Compute gradients using **chain rule**.  
  - Update weights using **gradient descent**.  
- ðŸ”² Implement **learning rate schedules** (e.g., step decay, exponential decay).  
- ðŸ”² Add support for **SGD, Momentum, Adam, RMSprop** optimizers.  
- ðŸ”² Update README  

### **9. Loss Functions**  
- ðŸ”² Implement standard **loss functions**:  
  - Mean Squared Error (MSE) (for regression).  
  - Cross-Entropy Loss (for classification).  
  - Hinge Loss (for SVM-like models).  
- ðŸ”² Update README  

---

## **Phase 4: Regularization and Model Generalization**  
### **10. Regularization Techniques**  
- âœ… Implement **L1/L2 regularization (weight decay)**.  
- ðŸ”² Implement **dropout** to improve generalization.  
- ðŸ”² Update README  

### **11. Model Evaluation & Metrics**  
- ðŸ”² Implement **accuracy, precision, recall, F1-score, confusion matrix**.  
- ðŸ”² Support **validation & test splits** for training models properly.  
- ðŸ”² Update README  

---

## **Phase 5: Performance & Benchmarking**  
### **12. Benchmarking and Performance Analysis**  
- ðŸ”² Implement benchmarking tests for **Vector and Matrix operations**.  
- ðŸ”² Compare performance of **closed-form vs gradient descent regression**.  
- ðŸ”² Measure training speed and efficiency for **Logistic Regression, MLPs, SVMs**.  
- ðŸ”² Profile memory usage for **various models**.  
- ðŸ”² Update README  

### **13. Optimization & Parallelization**  
- ðŸ”² Improve any moodels not passing benchmarks. 
- ðŸ”² Implement **automatic differentiation** for gradient computation.  
- ðŸ”² Optimize matrix computations using **parallelization (multi-threading or SIMD)**.  
- ðŸ”² Consider **GPU acceleration** with Rust libraries like `wgpu` or `cust` (CUDA bindings).  
- ðŸ”² Update README  


### **14.A Import RustBrain, Complete Real Data Applications**  
- ðŸ”² Run MLP on MNIST data 
- ðŸ”² Run logistic regression 

### **14.B Advanced Usages for Experimental Applications**  
- ðŸ”² Create a Transformer 
- ðŸ”² Create a Vision Transformer